<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Data Engineering on jamesm.blog</title>
    <link>https://jamesm.blog/data-engineering/</link>
    <description>Recent content in Data Engineering on jamesm.blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 25 Feb 2024 21:30:25 +0100</lastBuildDate><atom:link href="https://jamesm.blog/data-engineering/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Databricks Training &amp; Certification</title>
      <link>https://jamesm.blog/data-engineering/databricks-training-certification/</link>
      <pubDate>Sun, 25 Feb 2024 21:30:25 +0100</pubDate>
      
      <guid>https://jamesm.blog/data-engineering/databricks-training-certification/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/&#34;&gt;Coursera&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/databricks&#34;&gt;Databricks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.databricks.com/&#34;&gt;Databricks&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.databricks.com/learn/training/home&#34;&gt;Databricks Training &amp;amp; Certification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.databricks.com/learn&#34;&gt;Learn&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.databricks.com/training/catalog&#34;&gt;Learning Library&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.datacamp.com/&#34;&gt;DataCamp&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.datacamp.com/tutorial/comprehensive-guide-to-databricks-lakehouse-ai&#34;&gt;A Comprehensive Guide to Databricks Lakehouse AI For Data Scientists&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.datacamp.com/tutorial/introduction-to-databricks&#34;&gt;Databricks Tutorial: 7 Must-know Concepts For Any Data Specialist&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.datacamp.com/courses/introduction-to-databricks&#34;&gt;Introduction to Databricks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.edx.org/&#34;&gt;edX&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.edx.org/school/databricks&#34;&gt;Databricks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.udemy.com/&#34;&gt;Udemy&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.udemy.com/course/databricks-certified-data-engineer-associate/&#34;&gt;Databricks Certified Data Engineer Associate&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.udemy.com/course/databricks-certified-data-engineer-professional/&#34;&gt;Databricks Certified Data Engineer Professional&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.whizlabs.com/&#34;&gt;Whizlabs&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Practice Tests
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.whizlabs.com/databricks-certified-associate-developer-apache-spark/&#34;&gt;Databricks Certified Associate Developer for Apache Spark (Python)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.whizlabs.com/databricks-certified-data-analyst-associate/&#34;&gt;Databricks Certified Data Analyst Associate Certification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.whizlabs.com/databricks-certified-data-engineer-associate/&#34;&gt;Databricks Certified Data Engineer Associate Certification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.whizlabs.com/databricks-certified-data-engineer-professional/&#34;&gt;Databricks Certified Data Engineer Professional Certification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.whizlabs.com/databricks-certified-machine-learning-associate/&#34;&gt;Databricks Certified Machine Learning Associate Certification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.whizlabs.com/databricks-certified-machine-learning-professional-certification/&#34;&gt;Databricks Certified Machine Learning Professional Certification&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
      <content:encoded><![CDATA[<ul>
<li><a href="https://www.coursera.org/">Coursera</a>
<ul>
<li><a href="https://www.coursera.org/databricks">Databricks</a></li>
</ul>
</li>
<li><a href="https://www.databricks.com/">Databricks</a>
<ul>
<li><a href="https://www.databricks.com/learn/training/home">Databricks Training &amp; Certification</a></li>
<li><a href="https://www.databricks.com/learn">Learn</a></li>
<li><a href="https://www.databricks.com/training/catalog">Learning Library</a></li>
</ul>
</li>
<li><a href="https://www.datacamp.com/">DataCamp</a>
<ul>
<li><a href="https://www.datacamp.com/tutorial/comprehensive-guide-to-databricks-lakehouse-ai">A Comprehensive Guide to Databricks Lakehouse AI For Data Scientists</a></li>
<li><a href="https://www.datacamp.com/tutorial/introduction-to-databricks">Databricks Tutorial: 7 Must-know Concepts For Any Data Specialist</a></li>
<li><a href="https://www.datacamp.com/courses/introduction-to-databricks">Introduction to Databricks</a></li>
</ul>
</li>
<li><a href="https://www.edx.org/">edX</a>
<ul>
<li><a href="https://www.edx.org/school/databricks">Databricks</a></li>
</ul>
</li>
<li><a href="https://www.udemy.com/">Udemy</a>
<ul>
<li><a href="https://www.udemy.com/course/databricks-certified-data-engineer-associate/">Databricks Certified Data Engineer Associate</a></li>
<li><a href="https://www.udemy.com/course/databricks-certified-data-engineer-professional/">Databricks Certified Data Engineer Professional</a></li>
</ul>
</li>
<li><a href="https://www.whizlabs.com/">Whizlabs</a>
<ul>
<li>Practice Tests
<ul>
<li><a href="https://www.whizlabs.com/databricks-certified-associate-developer-apache-spark/">Databricks Certified Associate Developer for Apache Spark (Python)</a></li>
<li><a href="https://www.whizlabs.com/databricks-certified-data-analyst-associate/">Databricks Certified Data Analyst Associate Certification</a></li>
<li><a href="https://www.whizlabs.com/databricks-certified-data-engineer-associate/">Databricks Certified Data Engineer Associate Certification</a></li>
<li><a href="https://www.whizlabs.com/databricks-certified-data-engineer-professional/">Databricks Certified Data Engineer Professional Certification</a></li>
<li><a href="https://www.whizlabs.com/databricks-certified-machine-learning-associate/">Databricks Certified Machine Learning Associate Certification</a></li>
<li><a href="https://www.whizlabs.com/databricks-certified-machine-learning-professional-certification/">Databricks Certified Machine Learning Professional Certification</a></li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Databricks Cheat Sheets</title>
      <link>https://jamesm.blog/data-engineering/databricks-cheatsheets/</link>
      <pubDate>Sun, 25 Feb 2024 21:00:25 +0100</pubDate>
      
      <guid>https://jamesm.blog/data-engineering/databricks-cheatsheets/</guid>
      <description>&lt;h2 id=&#34;databricks-notebook-commands&#34;&gt;Databricks Notebook Commands&lt;/h2&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Command&lt;/th&gt;
          &lt;th&gt;Purpose&lt;/th&gt;
          &lt;th&gt;Example&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;%config&lt;/td&gt;
          &lt;td&gt;Set configuration options for the notebook&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;%env&lt;/td&gt;
          &lt;td&gt;Set environment variables&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;%fs&lt;/td&gt;
          &lt;td&gt;Interact with the Databricks file system&lt;/td&gt;
          &lt;td&gt;%fs ls dbfs:/repo&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;%load&lt;/td&gt;
          &lt;td&gt;Loads the contents of a file into a cell&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;%lsmagic&lt;/td&gt;
          &lt;td&gt;List all magic commands&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;%jobs&lt;/td&gt;
          &lt;td&gt;Lists all running jobs&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;%matplotlib&lt;/td&gt;
          &lt;td&gt;sets up the matplotlib backend&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;%md&lt;/td&gt;
          &lt;td&gt;Write Markdown text&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;%pip&lt;/td&gt;
          &lt;td&gt;Install Python packages&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;%python&lt;/td&gt;
          &lt;td&gt;Executes python code&lt;/td&gt;
          &lt;td&gt;%python dbutils.fs.rm(&amp;quot;/user/hive/warehouse/test/&amp;quot;, True)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;%r&lt;/td&gt;
          &lt;td&gt;Execute R code&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;%reload&lt;/td&gt;
          &lt;td&gt;reloads module contents&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;%run&lt;/td&gt;
          &lt;td&gt;Executes a Python file or a notebook&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;%scala&lt;/td&gt;
          &lt;td&gt;Executes scala code&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;%sh&lt;/td&gt;
          &lt;td&gt;Executes shell commands on the cluster nodes&lt;/td&gt;
          &lt;td&gt;%sh git clone &lt;a href=&#34;https://github.com/repo/test&#34;&gt;https://github.com/repo/test&lt;/a&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;%sql&lt;/td&gt;
          &lt;td&gt;Executes SQL queries&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;%who&lt;/td&gt;
          &lt;td&gt;Lists all the variables in the current scope&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;accessing-files&#34;&gt;Accessing Files&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;/path/to/file&lt;/li&gt;
&lt;li&gt;dbfs:/path/to/file&lt;/li&gt;
&lt;li&gt;file:/path/to/file&lt;/li&gt;
&lt;li&gt;s3://path/to/file&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;copying-files&#34;&gt;Copying Files&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;%fs cp file:/&amp;lt;path&amp;gt; /Volumes/&amp;lt;catalog&amp;gt;/&amp;lt;schema&amp;gt;/&amp;lt;volume&amp;gt;/&amp;lt;path&amp;gt;

%python dbutils.fs.cp(&amp;#34;file:/&amp;lt;path&amp;gt;&amp;#34;, &amp;#34;/Volumes/&amp;lt;catalog&amp;gt;/&amp;lt;schema&amp;gt;/&amp;lt;volume&amp;gt;/&amp;lt;path&amp;gt;&amp;#34;)
%python dbutils.fs.cp(&amp;#34;file:/databricks/driver/test&amp;#34;, &amp;#34;dbfs:/repo&amp;#34;, True)

%sh cp /&amp;lt;path&amp;gt; /Volumes/&amp;lt;catalog&amp;gt;/&amp;lt;schema&amp;gt;/&amp;lt;volume&amp;gt;/&amp;lt;path&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;sql-statements-ddl&#34;&gt;SQL Statements (DDL)&lt;/h2&gt;
&lt;h3 id=&#34;create--use-schema&#34;&gt;Create &amp;amp; Use Schema&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;CREATE SCHEMA test;
CREATE SCHEMA custom LOCATION &amp;#39;dbfs:/custom&amp;#39;;

USE SCHEMA test;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;create-table&#34;&gt;Create Table&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;CREATE TABLE test(col1 INT, col2 STRING, col3 STRING, col4 BIGINT, col5 INT, col6 FLOAT);
CREATE TABLE test AS SELECT * EXCEPT (_rescued_data) FROM read_files(&amp;#39;/repo/data/test.csv&amp;#39;);
CREATE TABLE test USING CSV LOCATION &amp;#39;/repo/data/test.csv&amp;#39;;
CREATE TABLE test USING CSV OPTIONS (header=&amp;#34;true&amp;#34;) LOCATION &amp;#39;/repo/data/test.csv&amp;#39;;
CREATE TABLE test AS SELECT * EXCEPT (_rescued_data) FROM read_files(&amp;#39;/repo/data/test.csv&amp;#39;);
CREATE TABLE test AS ...
CREATE TABLE test USING ...

CREATE TABLE test(id INT, title STRING, col1 STRING, publish_time BIGINT, pages INT, price FLOAT)
COMMENT &amp;#39;This is comment for the table itself&amp;#39;;

CREATE TABLE test AS
SELECT * EXCEPT (_rescued_data)
FROM read_files(&amp;#39;/repo/data/test.json&amp;#39;, format =&amp;gt; &amp;#39;json&amp;#39;);

CREATE TABLE test_raw AS
SELECT * EXCEPT (_rescued_data)
FROM read_files(&amp;#39;/repo/data/test.csv&amp;#39;, sep =&amp;gt; &amp;#39;;&amp;#39;);

CREATE TABLE custom_table_test LOCATION &amp;#39;dbfs:/custom-table&amp;#39;
AS SELECT * EXCEPT (_rescued_data) FROM read_files(&amp;#39;/repo/data/test.csv&amp;#39;);

CREATE TABLE test PARTITIONED BY (col1)
AS SELECT * EXCEPT (_rescued_data) FROM read_files(&amp;#39;/repo/data/test.csv&amp;#39;)

CREATE TABLE users(
firstname STRING,
lastname STRING,
full_name STRING GENERATED ALWAYS AS (concat(firstname, &amp;#39; &amp;#39;, lastname))
);

CREATE OR REPLACE TABLE test AS SELECT * EXCEPT (_rescued_data) FROM read_files(&amp;#39;/repo/data/test.csv&amp;#39;);
CREATE OR REPLACE TABLE test AS SELECT * FROM json.`/repo/data/test.json`;
CREATE OR REPLACE TABLE test AS SELECT * FROM read_files(&amp;#39;/repo/data/test.csv&amp;#39;);
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;create-view&#34;&gt;Create View&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;CREATE VIEW view_test
AS SELECT * FROM test WHERE col1 = &amp;#39;test&amp;#39;;

CREATE VIEW view_test
AS SELECT col1, col1
FROM test
JOIN test2 ON test.col2 == test2.col2;

CREATE TEMP VIEW temp_test
AS SELECT * FROM test WHERE col1 = &amp;#39;test&amp;#39;;

CREATE TEMP VIEW temp_test
AS SELECT * FROM read_files(&amp;#39;/repo/data/test.csv&amp;#39;);

CREATE GLOBAL TEMP VIEW view_test
AS SELECT * FROM test WHERE col1 = &amp;#39;test&amp;#39;;
SELECT * FROM global_temp.view_test;

CREATE TEMP VIEW jdbc_example USING JDBC
OPTIONS (
url &amp;#34;&amp;lt;jdbc-url&amp;gt;&amp;#34;,
dbtable &amp;#34;&amp;lt;table-name&amp;gt;&amp;#34;,
user &amp;#39;&amp;lt;username&amp;gt;&amp;#39;,
password &amp;#39;&amp;lt;password&amp;gt;&amp;#39;);

CREATE OR REPLACE TEMP VIEW test AS SELECT * FROM delta.`&amp;lt;logpath&amp;gt;`;

CREATE VIEW event_log_raw AS SELECT * FROM event_log(&amp;#34;&amp;lt;pipeline-id&amp;gt;&amp;#34;);

CREATE OR REPLACE TEMP VIEW test_view
AS SELECT test.col1 AS col1 FROM test_table
WHERE col1 = &amp;#39;value1&amp;#39; ORDER BY timestamp DESC LIMIT 1;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;drop&#34;&gt;Drop&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;DROP TABLE test;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;describe&#34;&gt;Describe&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;SHOW TABLES;

DESCRIBE EXTENDED test;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;sql-statements-dml&#34;&gt;SQL Statements (DML)&lt;/h2&gt;
&lt;h3 id=&#34;select&#34;&gt;Select&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;SELECT * FROM csv.`/repo/data/test.csv`;
SELECT * FROM read_files(&amp;#39;/repo/data/test.csv&amp;#39;);
SELECT * FROM read_files(&amp;#39;/repo/data/test.csv&amp;#39;, format =&amp;gt; &amp;#39;csv&amp;#39;, header =&amp;gt; &amp;#39;true&amp;#39;, sep =&amp;gt; &amp;#39;,&amp;#39;)
SELECT * FROM json.`/repo/data/test.json`;
SELECT * FROM json.`/repo/data/*.json`;
SELECT * FROM test WHERE year(from_unixtime(test_time)) &amp;gt; 1900;
SELECT * FROM test WHERE title LIKE &amp;#39;%a%&amp;#39;
SELECT * FROM test WHERE title LIKE &amp;#39;a%&amp;#39;
SELECT * FROM test WHERE title LIKE &amp;#39;%a&amp;#39;
SELECT * FROM test TIMESTAMP AS OF &amp;#39;2024-01-01T00:00:00.000Z&amp;#39;;
SELECT * FROM test VERSION AS OF 2;
SELECT * FROM test@v2;
SELECT * FROM event_log(&amp;#34;&amp;lt;pipeline-id&amp;gt;&amp;#34;);

SELECT count(*) FROM VALUES (NULL), (10), (10) AS example(col);
SELECT count(col) FROM VALUES (NULL), (10), (10) AS example(col);
SELECT count_if(col1 = &amp;#39;test&amp;#39;) FROM test;
SELECT from_unixtime(test_time) FROM test;
SELECT cast(test_time / 1 AS timestamp) FROM test;
SELECT cast(cast(test_time AS BIGINT) AS timestamp) FROM test;
SELECT element.sub_element FROM test;
SELECT flatten(array(array(1, 2), array(3, 4)));

SELECT * FROM (
  SELECT col1, col2 FROM test
) 
PIVOT (
  sum(col1) for col2 in (&amp;#39;item1&amp;#39;,&amp;#39;item2&amp;#39;)
);

SELECT *, CASE
WHEN col1 &amp;gt; 10 THEN &amp;#39;value1&amp;#39;
ELSE &amp;#39;value2&amp;#39;
END
FROM test;

SELECT * FROM test ORDER BY (CASE
WHEN col1 &amp;gt; 10 THEN col2
ELSE col3
END);

WITH t(col1, col2) AS (SELECT 1, 2)
SELECT * FROM t WHERE col1 = 1;

SELECT details:flow_definition.output_dataset as output_dataset,
       details:flow_definition.input_datasets as input_dataset
FROM   event_log_raw, latest_update
WHERE  event_type = &amp;#39;flow_definition&amp;#39; AND origin.update_id = latest_update.id;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;insert&#34;&gt;Insert&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;INSERT OVERWRITE test SELECT * FROM read_files(&amp;#39;/repo/data/test.csv&amp;#39;);

INSERT INTO test(col1, col2) VALUES (&amp;#39;value1&amp;#39;, &amp;#39;value2&amp;#39;);
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;merge-into&#34;&gt;Merge Into&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;MERGE INTO test USING test_to_delete
ON test.col1 = test_to_delete.col1
WHEN MATCHED THEN DELETE;

MERGE INTO test USING test_to_update
ON test.col1 = test_to_update.col1
WHEN MATCHED THEN UPDATE SET *;

MERGE INTO test USING test_to_insert
ON test.col1 = test_to_insert.col1
WHEN NOT MATCHED THEN INSERT *;
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;copy-into&#34;&gt;Copy Into&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;COPY INTO test
FROM &amp;#39;/repo/data&amp;#39;
FILEFORMAT = CSV
FILES = (&amp;#39;test.csv&amp;#39;)
FORMAT_OPTIONS(&amp;#39;header&amp;#39; = &amp;#39;true&amp;#39;, &amp;#39;inferSchema&amp;#39; = &amp;#39;true&amp;#39;);
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;delta-lake-statements&#34;&gt;Delta Lake Statements&lt;/h2&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;DESCRIBE HISTORY test;
DESCRIBE HISTORY test LIMIT 1;

INSERT INTO test SELECT * FROM test@v2 WHERE id = 3;

OPTIMIZE test;
OPTIMIZE test ZORDER BY col1;

RESTORE TABLE test TO VERSION AS OF 0;

SELECT * FROM test TIMESTAMP AS OF &amp;#39;2024-01-01T00:00:00.000Z&amp;#39;;
SELECT * FROM test VERSION AS OF 2;
SELECT * FROM test@v2;

VACUUM test;
VACUUM test RETAIN 240 HOURS;

%fs ls dbfs:/user/hive/warehouse/test/_delta_log
%python spark.conf.set(&amp;#34;spark.databricks.delta.retentionDurationCheck.enabled&amp;#34;, &amp;#34;false&amp;#34;)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;delta-live-table-statements&#34;&gt;Delta Live Table Statements&lt;/h2&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;CREATE OR REFRESH LIVE TABLE test_raw
AS SELECT * FROM json.`/repo/data/test.json`;

CREATE OR REFRESH STREAMING TABLE test
AS SELECT * FROM STREAM read_files(&amp;#39;/repo/data/test*.json&amp;#39;);

CREATE OR REFRESH LIVE TABLE test_cleaned
AS SELECT col1, col2, col3, col4 FROM live.test_raw;

CREATE OR REFRESH LIVE TABLE recent_test
AS SELECT col1, col2 FROM live.test2 ORDER BY creation_time DESC LIMIT 10;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;fuctions&#34;&gt;Fuctions&lt;/h2&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;CREATE OR REPLACE FUNCTION test_function(temp DOUBLE)
RETURNS DOUBLE
RETURN (col1 - 10);
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;auto-loader&#34;&gt;Auto Loader&lt;/h2&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;%python

spark.readStream.format(&amp;#34;cloudFiles&amp;#34;)\
  .option(&amp;#34;cloudFiles.format&amp;#34;, &amp;#34;json&amp;#34;)\
  .option(&amp;#34;cloudFiles.schemaLocation&amp;#34;, &amp;#34;/autoloader-schema&amp;#34;)\
  .option(&amp;#34;pathGlobFilter&amp;#34;, &amp;#34;test*.json&amp;#34;)\
  .load(&amp;#34;/repo/data&amp;#34;)\
  .writeStream\
  .option(&amp;#34;mergeSchema&amp;#34;, &amp;#34;true&amp;#34;)\
  .option(&amp;#34;checkpointLocation&amp;#34;, &amp;#34;/autoloader-checkpoint&amp;#34;)\
  .start(&amp;#34;demo&amp;#34;)

%fs head /autoloader-schema/_schemas/0

CREATE OR REFRESH STREAMING TABLE test
AS SELECT * FROM
cloud_files(
&amp;#39;/repo/data&amp;#39;,
&amp;#39;json&amp;#39;,
map(&amp;#34;cloudFiles.inferColumnTypes&amp;#34;, &amp;#34;true&amp;#34;, &amp;#34;pathGlobFilter&amp;#34;, &amp;#34;test*.json&amp;#34;)
);

CONSTRAINT positive_timestamp EXPECT (creation_time &amp;gt; 0)
CONSTRAINT positive_timestamp EXPECT (creation_time &amp;gt; 0) ON VIOLATION DROP ROW
CONSTRAINT positive_timestamp EXPECT (creation_time &amp;gt; 0) ON VIOLATION FAIL UPDATE
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;cdc-statements&#34;&gt;CDC Statements&lt;/h2&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;APPLY CHANGES INTO live.target
  FROM stream(live.cdc_source)
  KEYS (col1)
  APPLY AS DELETE WHEN col2 = &amp;#34;DELETE&amp;#34;
  SEQUENCE BY col3
  COLUMNS * EXCEPT (col);
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;security-statements&#34;&gt;Security Statements&lt;/h2&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;GRANT &amp;lt;privilege&amp;gt; ON &amp;lt;object_type&amp;gt; &amp;lt;object_name&amp;gt; TO &amp;lt;user_or_group&amp;gt;;
GRANT SELECT ON TABLE test TO `databricks@degols.net`;

REVOKE &amp;lt;privilege&amp;gt; ON &amp;lt;object_type&amp;gt; &amp;lt;object_name&amp;gt; FROM `test@gmail.com&amp;#39;;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;links&#34;&gt;Links&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/en/index.html&#34;&gt;Databricks&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/en/sql/language-manual/index.html&#34;&gt;SQL Language Reference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/en/getting-started/best-practices.html&#34;&gt;Cheat Sheets&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/en/cheat-sheet/compute.html&#34;&gt;Compute creation cheat sheet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/en/cheat-sheet/administration.html&#34;&gt;Platform administration cheat sheet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/en/cheat-sheet/jobs.html&#34;&gt;Production job scheduling cheat sheet&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/en/getting-started/best-practices.html&#34;&gt;Best Practices&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/en/delta/best-practices.html&#34;&gt;Delta Lake best practices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/en/machine-learning/automl-hyperparam-tuning/hyperopt-best-practices.html&#34;&gt;Hyperparameter tuning with Hyperopt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/en/machine-learning/train-model/dl-best-practices.html&#34;&gt;Deep learning in Databricks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/en/machine-learning/mlops/mlops-workflow.html&#34;&gt;Recommendations for MLOps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/en/data-governance/unity-catalog/best-practices.html&#34;&gt;Unity Catalog best practices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/en/compute/cluster-config-best-practices.html&#34;&gt;Cluster configuration best practices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/en/compute/pool-best-practices.html&#34;&gt;Instance pool configuration best practices&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Other
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://mayur-saparia7.medium.com/databricks-cheat-sheet-1-a0d3e0f70065&#34;&gt;Databricks Cheat Sheet 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://grabngoinfo.com/databricks-notebook-markdown-cheat-sheet/&#34;&gt;Databricks Notebook Markdown Cheat Sheet&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
      <content:encoded><![CDATA[<h2 id="databricks-notebook-commands">Databricks Notebook Commands</h2>
<table>
  <thead>
      <tr>
          <th>Command</th>
          <th>Purpose</th>
          <th>Example</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>%config</td>
          <td>Set configuration options for the notebook</td>
          <td></td>
      </tr>
      <tr>
          <td>%env</td>
          <td>Set environment variables</td>
          <td></td>
      </tr>
      <tr>
          <td>%fs</td>
          <td>Interact with the Databricks file system</td>
          <td>%fs ls dbfs:/repo</td>
      </tr>
      <tr>
          <td>%load</td>
          <td>Loads the contents of a file into a cell</td>
          <td></td>
      </tr>
      <tr>
          <td>%lsmagic</td>
          <td>List all magic commands</td>
          <td></td>
      </tr>
      <tr>
          <td>%jobs</td>
          <td>Lists all running jobs</td>
          <td></td>
      </tr>
      <tr>
          <td>%matplotlib</td>
          <td>sets up the matplotlib backend</td>
          <td></td>
      </tr>
      <tr>
          <td>%md</td>
          <td>Write Markdown text</td>
          <td></td>
      </tr>
      <tr>
          <td>%pip</td>
          <td>Install Python packages</td>
          <td></td>
      </tr>
      <tr>
          <td>%python</td>
          <td>Executes python code</td>
          <td>%python dbutils.fs.rm(&quot;/user/hive/warehouse/test/&quot;, True)</td>
      </tr>
      <tr>
          <td>%r</td>
          <td>Execute R code</td>
          <td></td>
      </tr>
      <tr>
          <td>%reload</td>
          <td>reloads module contents</td>
          <td></td>
      </tr>
      <tr>
          <td>%run</td>
          <td>Executes a Python file or a notebook</td>
          <td></td>
      </tr>
      <tr>
          <td>%scala</td>
          <td>Executes scala code</td>
          <td></td>
      </tr>
      <tr>
          <td>%sh</td>
          <td>Executes shell commands on the cluster nodes</td>
          <td>%sh git clone <a href="https://github.com/repo/test">https://github.com/repo/test</a></td>
      </tr>
      <tr>
          <td>%sql</td>
          <td>Executes SQL queries</td>
          <td></td>
      </tr>
      <tr>
          <td>%who</td>
          <td>Lists all the variables in the current scope</td>
          <td></td>
      </tr>
  </tbody>
</table>
<h3 id="accessing-files">Accessing Files</h3>
<ul>
<li>/path/to/file</li>
<li>dbfs:/path/to/file</li>
<li>file:/path/to/file</li>
<li>s3://path/to/file</li>
</ul>
<h3 id="copying-files">Copying Files</h3>
<pre tabindex="0"><code>%fs cp file:/&lt;path&gt; /Volumes/&lt;catalog&gt;/&lt;schema&gt;/&lt;volume&gt;/&lt;path&gt;

%python dbutils.fs.cp(&#34;file:/&lt;path&gt;&#34;, &#34;/Volumes/&lt;catalog&gt;/&lt;schema&gt;/&lt;volume&gt;/&lt;path&gt;&#34;)
%python dbutils.fs.cp(&#34;file:/databricks/driver/test&#34;, &#34;dbfs:/repo&#34;, True)

%sh cp /&lt;path&gt; /Volumes/&lt;catalog&gt;/&lt;schema&gt;/&lt;volume&gt;/&lt;path&gt;
</code></pre><h2 id="sql-statements-ddl">SQL Statements (DDL)</h2>
<h3 id="create--use-schema">Create &amp; Use Schema</h3>
<pre tabindex="0"><code>CREATE SCHEMA test;
CREATE SCHEMA custom LOCATION &#39;dbfs:/custom&#39;;

USE SCHEMA test;
</code></pre><h3 id="create-table">Create Table</h3>
<pre tabindex="0"><code>CREATE TABLE test(col1 INT, col2 STRING, col3 STRING, col4 BIGINT, col5 INT, col6 FLOAT);
CREATE TABLE test AS SELECT * EXCEPT (_rescued_data) FROM read_files(&#39;/repo/data/test.csv&#39;);
CREATE TABLE test USING CSV LOCATION &#39;/repo/data/test.csv&#39;;
CREATE TABLE test USING CSV OPTIONS (header=&#34;true&#34;) LOCATION &#39;/repo/data/test.csv&#39;;
CREATE TABLE test AS SELECT * EXCEPT (_rescued_data) FROM read_files(&#39;/repo/data/test.csv&#39;);
CREATE TABLE test AS ...
CREATE TABLE test USING ...

CREATE TABLE test(id INT, title STRING, col1 STRING, publish_time BIGINT, pages INT, price FLOAT)
COMMENT &#39;This is comment for the table itself&#39;;

CREATE TABLE test AS
SELECT * EXCEPT (_rescued_data)
FROM read_files(&#39;/repo/data/test.json&#39;, format =&gt; &#39;json&#39;);

CREATE TABLE test_raw AS
SELECT * EXCEPT (_rescued_data)
FROM read_files(&#39;/repo/data/test.csv&#39;, sep =&gt; &#39;;&#39;);

CREATE TABLE custom_table_test LOCATION &#39;dbfs:/custom-table&#39;
AS SELECT * EXCEPT (_rescued_data) FROM read_files(&#39;/repo/data/test.csv&#39;);

CREATE TABLE test PARTITIONED BY (col1)
AS SELECT * EXCEPT (_rescued_data) FROM read_files(&#39;/repo/data/test.csv&#39;)

CREATE TABLE users(
firstname STRING,
lastname STRING,
full_name STRING GENERATED ALWAYS AS (concat(firstname, &#39; &#39;, lastname))
);

CREATE OR REPLACE TABLE test AS SELECT * EXCEPT (_rescued_data) FROM read_files(&#39;/repo/data/test.csv&#39;);
CREATE OR REPLACE TABLE test AS SELECT * FROM json.`/repo/data/test.json`;
CREATE OR REPLACE TABLE test AS SELECT * FROM read_files(&#39;/repo/data/test.csv&#39;);
</code></pre><h3 id="create-view">Create View</h3>
<pre tabindex="0"><code>CREATE VIEW view_test
AS SELECT * FROM test WHERE col1 = &#39;test&#39;;

CREATE VIEW view_test
AS SELECT col1, col1
FROM test
JOIN test2 ON test.col2 == test2.col2;

CREATE TEMP VIEW temp_test
AS SELECT * FROM test WHERE col1 = &#39;test&#39;;

CREATE TEMP VIEW temp_test
AS SELECT * FROM read_files(&#39;/repo/data/test.csv&#39;);

CREATE GLOBAL TEMP VIEW view_test
AS SELECT * FROM test WHERE col1 = &#39;test&#39;;
SELECT * FROM global_temp.view_test;

CREATE TEMP VIEW jdbc_example USING JDBC
OPTIONS (
url &#34;&lt;jdbc-url&gt;&#34;,
dbtable &#34;&lt;table-name&gt;&#34;,
user &#39;&lt;username&gt;&#39;,
password &#39;&lt;password&gt;&#39;);

CREATE OR REPLACE TEMP VIEW test AS SELECT * FROM delta.`&lt;logpath&gt;`;

CREATE VIEW event_log_raw AS SELECT * FROM event_log(&#34;&lt;pipeline-id&gt;&#34;);

CREATE OR REPLACE TEMP VIEW test_view
AS SELECT test.col1 AS col1 FROM test_table
WHERE col1 = &#39;value1&#39; ORDER BY timestamp DESC LIMIT 1;
</code></pre><h3 id="drop">Drop</h3>
<pre tabindex="0"><code>DROP TABLE test;
</code></pre><h3 id="describe">Describe</h3>
<pre tabindex="0"><code>SHOW TABLES;

DESCRIBE EXTENDED test;
</code></pre><h2 id="sql-statements-dml">SQL Statements (DML)</h2>
<h3 id="select">Select</h3>
<pre tabindex="0"><code>SELECT * FROM csv.`/repo/data/test.csv`;
SELECT * FROM read_files(&#39;/repo/data/test.csv&#39;);
SELECT * FROM read_files(&#39;/repo/data/test.csv&#39;, format =&gt; &#39;csv&#39;, header =&gt; &#39;true&#39;, sep =&gt; &#39;,&#39;)
SELECT * FROM json.`/repo/data/test.json`;
SELECT * FROM json.`/repo/data/*.json`;
SELECT * FROM test WHERE year(from_unixtime(test_time)) &gt; 1900;
SELECT * FROM test WHERE title LIKE &#39;%a%&#39;
SELECT * FROM test WHERE title LIKE &#39;a%&#39;
SELECT * FROM test WHERE title LIKE &#39;%a&#39;
SELECT * FROM test TIMESTAMP AS OF &#39;2024-01-01T00:00:00.000Z&#39;;
SELECT * FROM test VERSION AS OF 2;
SELECT * FROM test@v2;
SELECT * FROM event_log(&#34;&lt;pipeline-id&gt;&#34;);

SELECT count(*) FROM VALUES (NULL), (10), (10) AS example(col);
SELECT count(col) FROM VALUES (NULL), (10), (10) AS example(col);
SELECT count_if(col1 = &#39;test&#39;) FROM test;
SELECT from_unixtime(test_time) FROM test;
SELECT cast(test_time / 1 AS timestamp) FROM test;
SELECT cast(cast(test_time AS BIGINT) AS timestamp) FROM test;
SELECT element.sub_element FROM test;
SELECT flatten(array(array(1, 2), array(3, 4)));

SELECT * FROM (
  SELECT col1, col2 FROM test
) 
PIVOT (
  sum(col1) for col2 in (&#39;item1&#39;,&#39;item2&#39;)
);

SELECT *, CASE
WHEN col1 &gt; 10 THEN &#39;value1&#39;
ELSE &#39;value2&#39;
END
FROM test;

SELECT * FROM test ORDER BY (CASE
WHEN col1 &gt; 10 THEN col2
ELSE col3
END);

WITH t(col1, col2) AS (SELECT 1, 2)
SELECT * FROM t WHERE col1 = 1;

SELECT details:flow_definition.output_dataset as output_dataset,
       details:flow_definition.input_datasets as input_dataset
FROM   event_log_raw, latest_update
WHERE  event_type = &#39;flow_definition&#39; AND origin.update_id = latest_update.id;
</code></pre><h3 id="insert">Insert</h3>
<pre tabindex="0"><code>INSERT OVERWRITE test SELECT * FROM read_files(&#39;/repo/data/test.csv&#39;);

INSERT INTO test(col1, col2) VALUES (&#39;value1&#39;, &#39;value2&#39;);
</code></pre><h3 id="merge-into">Merge Into</h3>
<pre tabindex="0"><code>MERGE INTO test USING test_to_delete
ON test.col1 = test_to_delete.col1
WHEN MATCHED THEN DELETE;

MERGE INTO test USING test_to_update
ON test.col1 = test_to_update.col1
WHEN MATCHED THEN UPDATE SET *;

MERGE INTO test USING test_to_insert
ON test.col1 = test_to_insert.col1
WHEN NOT MATCHED THEN INSERT *;
</code></pre><h3 id="copy-into">Copy Into</h3>
<pre tabindex="0"><code>COPY INTO test
FROM &#39;/repo/data&#39;
FILEFORMAT = CSV
FILES = (&#39;test.csv&#39;)
FORMAT_OPTIONS(&#39;header&#39; = &#39;true&#39;, &#39;inferSchema&#39; = &#39;true&#39;);
</code></pre><h2 id="delta-lake-statements">Delta Lake Statements</h2>
<pre tabindex="0"><code>DESCRIBE HISTORY test;
DESCRIBE HISTORY test LIMIT 1;

INSERT INTO test SELECT * FROM test@v2 WHERE id = 3;

OPTIMIZE test;
OPTIMIZE test ZORDER BY col1;

RESTORE TABLE test TO VERSION AS OF 0;

SELECT * FROM test TIMESTAMP AS OF &#39;2024-01-01T00:00:00.000Z&#39;;
SELECT * FROM test VERSION AS OF 2;
SELECT * FROM test@v2;

VACUUM test;
VACUUM test RETAIN 240 HOURS;

%fs ls dbfs:/user/hive/warehouse/test/_delta_log
%python spark.conf.set(&#34;spark.databricks.delta.retentionDurationCheck.enabled&#34;, &#34;false&#34;)
</code></pre><h2 id="delta-live-table-statements">Delta Live Table Statements</h2>
<pre tabindex="0"><code>CREATE OR REFRESH LIVE TABLE test_raw
AS SELECT * FROM json.`/repo/data/test.json`;

CREATE OR REFRESH STREAMING TABLE test
AS SELECT * FROM STREAM read_files(&#39;/repo/data/test*.json&#39;);

CREATE OR REFRESH LIVE TABLE test_cleaned
AS SELECT col1, col2, col3, col4 FROM live.test_raw;

CREATE OR REFRESH LIVE TABLE recent_test
AS SELECT col1, col2 FROM live.test2 ORDER BY creation_time DESC LIMIT 10;
</code></pre><h2 id="fuctions">Fuctions</h2>
<pre tabindex="0"><code>CREATE OR REPLACE FUNCTION test_function(temp DOUBLE)
RETURNS DOUBLE
RETURN (col1 - 10);
</code></pre><h2 id="auto-loader">Auto Loader</h2>
<pre tabindex="0"><code>%python

spark.readStream.format(&#34;cloudFiles&#34;)\
  .option(&#34;cloudFiles.format&#34;, &#34;json&#34;)\
  .option(&#34;cloudFiles.schemaLocation&#34;, &#34;/autoloader-schema&#34;)\
  .option(&#34;pathGlobFilter&#34;, &#34;test*.json&#34;)\
  .load(&#34;/repo/data&#34;)\
  .writeStream\
  .option(&#34;mergeSchema&#34;, &#34;true&#34;)\
  .option(&#34;checkpointLocation&#34;, &#34;/autoloader-checkpoint&#34;)\
  .start(&#34;demo&#34;)

%fs head /autoloader-schema/_schemas/0

CREATE OR REFRESH STREAMING TABLE test
AS SELECT * FROM
cloud_files(
&#39;/repo/data&#39;,
&#39;json&#39;,
map(&#34;cloudFiles.inferColumnTypes&#34;, &#34;true&#34;, &#34;pathGlobFilter&#34;, &#34;test*.json&#34;)
);

CONSTRAINT positive_timestamp EXPECT (creation_time &gt; 0)
CONSTRAINT positive_timestamp EXPECT (creation_time &gt; 0) ON VIOLATION DROP ROW
CONSTRAINT positive_timestamp EXPECT (creation_time &gt; 0) ON VIOLATION FAIL UPDATE
</code></pre><h2 id="cdc-statements">CDC Statements</h2>
<pre tabindex="0"><code>APPLY CHANGES INTO live.target
  FROM stream(live.cdc_source)
  KEYS (col1)
  APPLY AS DELETE WHEN col2 = &#34;DELETE&#34;
  SEQUENCE BY col3
  COLUMNS * EXCEPT (col);
</code></pre><h2 id="security-statements">Security Statements</h2>
<pre tabindex="0"><code>GRANT &lt;privilege&gt; ON &lt;object_type&gt; &lt;object_name&gt; TO &lt;user_or_group&gt;;
GRANT SELECT ON TABLE test TO `databricks@degols.net`;

REVOKE &lt;privilege&gt; ON &lt;object_type&gt; &lt;object_name&gt; FROM `test@gmail.com&#39;;
</code></pre><h2 id="links">Links</h2>
<ul>
<li><a href="https://docs.databricks.com/en/index.html">Databricks</a>
<ul>
<li><a href="https://docs.databricks.com/en/sql/language-manual/index.html">SQL Language Reference</a></li>
<li><a href="https://docs.databricks.com/en/getting-started/best-practices.html">Cheat Sheets</a>
<ul>
<li><a href="https://docs.databricks.com/en/cheat-sheet/compute.html">Compute creation cheat sheet</a></li>
<li><a href="https://docs.databricks.com/en/cheat-sheet/administration.html">Platform administration cheat sheet</a></li>
<li><a href="https://docs.databricks.com/en/cheat-sheet/jobs.html">Production job scheduling cheat sheet</a></li>
</ul>
</li>
<li><a href="https://docs.databricks.com/en/getting-started/best-practices.html">Best Practices</a>
<ul>
<li><a href="https://docs.databricks.com/en/delta/best-practices.html">Delta Lake best practices</a></li>
<li><a href="https://docs.databricks.com/en/machine-learning/automl-hyperparam-tuning/hyperopt-best-practices.html">Hyperparameter tuning with Hyperopt</a></li>
<li><a href="https://docs.databricks.com/en/machine-learning/train-model/dl-best-practices.html">Deep learning in Databricks</a></li>
<li><a href="https://docs.databricks.com/en/machine-learning/mlops/mlops-workflow.html">Recommendations for MLOps</a></li>
<li><a href="https://docs.databricks.com/en/data-governance/unity-catalog/best-practices.html">Unity Catalog best practices</a></li>
<li><a href="https://docs.databricks.com/en/compute/cluster-config-best-practices.html">Cluster configuration best practices</a></li>
<li><a href="https://docs.databricks.com/en/compute/pool-best-practices.html">Instance pool configuration best practices</a></li>
</ul>
</li>
</ul>
</li>
<li>Other
<ul>
<li><a href="https://mayur-saparia7.medium.com/databricks-cheat-sheet-1-a0d3e0f70065">Databricks Cheat Sheet 1</a></li>
<li><a href="https://grabngoinfo.com/databricks-notebook-markdown-cheat-sheet/">Databricks Notebook Markdown Cheat Sheet</a></li>
</ul>
</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>List of Data Engineering &amp; Data Science Courses</title>
      <link>https://jamesm.blog/data-engineering/data-engineering-science-courses/</link>
      <pubDate>Sun, 25 Feb 2024 10:26:25 +0100</pubDate>
      
      <guid>https://jamesm.blog/data-engineering/data-engineering-science-courses/</guid>
      <description>&lt;h2 id=&#34;data-engineering&#34;&gt;Data Engineering&lt;/h2&gt;
&lt;h3 id=&#34;a-cloud-guru&#34;&gt;A Cloud Guru&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://acloudguru.com/course/apache-kafka-deep-dive/&#34;&gt;Apache Kafka Deep Dive&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://acloudguru.com/course/aws-certified-big-data-specialty/&#34;&gt;AWS Certified Big Data Specialty&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://acloudguru.com/course/google-certified-professional-data-engineer/&#34;&gt;Google Certified Professional Data Engineer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://acloudguru.com/course/microsoft-certified-azure-data-engineer-associate-dp-203/&#34;&gt;Microsoft Certified: Azure Data Engineer Associate (DP-203)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;coursera&#34;&gt;Coursera&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/introduction-to-data-engineering/&#34;&gt;Introduction to Data Engineering&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;datacamp&#34;&gt;DataCamp&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.datacamp.com/courses/building-data-engineering-pipelines-in-python/&#34;&gt;Building Data Engineering Pipelines in Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.datacamp.com/courses/database-design/&#34;&gt;Database Design&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.datacamp.com/courses/etl-in-python/&#34;&gt;ETL in Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.datacamp.com/courses/introduction-to-airflow-in-python/&#34;&gt;Introduction to Airflow in Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.datacamp.com/courses/introduction-to-data-engineering/&#34;&gt;Introduction to Data Engineering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.datacamp.com/courses/nosql-concepts/&#34;&gt;NoSQL Concepts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.datacamp.com/courses/streaming-concepts/&#34;&gt;Streaming Concepts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.datacamp.com/courses/understanding-data-engineering/&#34;&gt;Understanding Data Engineering&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;google&#34;&gt;Google&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cloudskillsboost.google/course_templates/53/&#34;&gt;Building Batch Data Pipelines on Google Cloud&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cloudskillsboost.google/course_templates/52/&#34;&gt;Building Resilient Streaming Analytics Systems on Google Cloud&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cloudskillsboost.google/course_templates/54/&#34;&gt;Modernizing Data Lakes and Data Warehouses with Google Cloud&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cloudskillsboost.google/course_templates/72/&#34;&gt;Preparing for the Google Cloud Professional Data Engineer Exam&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cloudskillsboost.google/course_templates/229/&#34;&gt;Serverless Data Processing with Dataflow: Develop Pipelines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cloudskillsboost.google/course_templates/218/&#34;&gt;Serverless Data Processing with Dataflow: Foundations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cloudskillsboost.google/course_templates/264/&#34;&gt;Serverless Data Processing with Dataflow: Operations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;udacity&#34;&gt;Udacity&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.udacity.com/course/data-engineer-nanodegree--nd027/&#34;&gt;How to Become a Data Engineer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;udemy&#34;&gt;Udemy&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.udemy.com/course/taming-big-data-with-apache-spark-hands-on/&#34;&gt;Taming Big Data with Apache Spark and Python - Hands On!&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;whizlabs&#34;&gt;Whizlabs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.whizlabs.com/learn/course/apache-kafka-fundamentals/297/&#34;&gt;Apache Kafka Fundamentals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.whizlabs.com/databricks-certified-associate-developer-apache-spark/&#34;&gt;Databricks Certified Associate Developer for Apache Spark (Python)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.whizlabs.com/databricks-certified-data-analyst-associate/&#34;&gt;Databricks Certified Data Analyst Associate Certification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.whizlabs.com/databricks-certified-data-engineer-associate/&#34;&gt;Databricks Certified Data Engineer Associate Certification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.whizlabs.com/databricks-certified-data-engineer-professional/&#34;&gt;Databricks Certified Data Engineer Professional Certification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.whizlabs.com/learn/course/snowflake-snowpro-core-certification/384/&#34;&gt;Snowflake SnowPro Core Certification&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;data-science&#34;&gt;Data Science&lt;/h2&gt;
&lt;h3 id=&#34;a-cloud-guru-1&#34;&gt;A Cloud Guru&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://acloudguru.com/course/introduction-to-machine-learning/&#34;&gt;Introduction to Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;coursera-1&#34;&gt;Coursera&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/specializations/data-science-with-databricks-for-data-analysts/&#34;&gt;Data Science with Databricks for Data Analysts Specialization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;datacamp-1&#34;&gt;DataCamp&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.datacamp.com/courses/introduction-to-data-science-in-python/&#34;&gt;Introduction to Data Science in Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.datacamp.com/courses/python-data-science-toolbox-part-1/&#34;&gt;Python Data Science Toolbox (Part 1)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;google-1&#34;&gt;Google&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://learndigital.withgoogle.com/digitalunlocked/course/data-science-foundations/&#34;&gt;Data Science Foundations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://learndigital.withgoogle.com/digitalunlocked/course/data-science-with-python/&#34;&gt;Data Science with Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cloudskillsboost.google/course_templates/3/&#34;&gt;Google Cloud Big Data and Machine Learning Fundamentals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://learndigital.withgoogle.com/digitalunlocked/course/intro-to-tensorflow-for-deep-learning/&#34;&gt;Intro to TensorFlow for Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://learndigital.withgoogle.com/digitalunlocked/course/learn-python-basics-for-data-analysis/&#34;&gt;Learn Python basics for data analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://learndigital.withgoogle.com/digitalunlocked/course/machine-learning-crash-course/&#34;&gt;Machine Learning Crash Course&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cloudskillsboost.google/course_templates/55/&#34;&gt;Smart Analytics, Machine Learning, and AI on Google Cloud&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;udemy-1&#34;&gt;Udemy&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.udemy.com/course/aws-machine-learning/&#34;&gt;AWS Certified Machine Learning Specialty 2023 - Hands On!&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;whizlabs-1&#34;&gt;Whizlabs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.whizlabs.com/learn/course/aws-certified-machine-learning-specialty/281/&#34;&gt;AWS Certified Machine Learning Specialty&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.whizlabs.com/databricks-certified-machine-learning-associate/&#34;&gt;Databricks Certified Machine Learning Associate Certification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.whizlabs.com/databricks-certified-machine-learning-professional-certification/&#34;&gt;Databricks Certified Machine Learning Professional Certification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.whizlabs.com/learn/course/data-science-with-python/379/&#34;&gt;Introduction to Data Science with Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.whizlabs.com/learn/course/tensorflow-for-deep-learning-with-python/1117/&#34;&gt;TensorFlow for Deep Learning with Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
      <content:encoded><![CDATA[<h2 id="data-engineering">Data Engineering</h2>
<h3 id="a-cloud-guru">A Cloud Guru</h3>
<ul>
<li><a href="https://acloudguru.com/course/apache-kafka-deep-dive/">Apache Kafka Deep Dive</a></li>
<li><a href="https://acloudguru.com/course/aws-certified-big-data-specialty/">AWS Certified Big Data Specialty</a></li>
<li><a href="https://acloudguru.com/course/google-certified-professional-data-engineer/">Google Certified Professional Data Engineer</a></li>
<li><a href="https://acloudguru.com/course/microsoft-certified-azure-data-engineer-associate-dp-203/">Microsoft Certified: Azure Data Engineer Associate (DP-203)</a></li>
</ul>
<h3 id="coursera">Coursera</h3>
<ul>
<li><a href="https://www.coursera.org/learn/introduction-to-data-engineering/">Introduction to Data Engineering</a></li>
</ul>
<h3 id="datacamp">DataCamp</h3>
<ul>
<li><a href="https://www.datacamp.com/courses/building-data-engineering-pipelines-in-python/">Building Data Engineering Pipelines in Python</a></li>
<li><a href="https://www.datacamp.com/courses/database-design/">Database Design</a></li>
<li><a href="https://www.datacamp.com/courses/etl-in-python/">ETL in Python</a></li>
<li><a href="https://www.datacamp.com/courses/introduction-to-airflow-in-python/">Introduction to Airflow in Python</a></li>
<li><a href="https://www.datacamp.com/courses/introduction-to-data-engineering/">Introduction to Data Engineering</a></li>
<li><a href="https://www.datacamp.com/courses/nosql-concepts/">NoSQL Concepts</a></li>
<li><a href="https://www.datacamp.com/courses/streaming-concepts/">Streaming Concepts</a></li>
<li><a href="https://www.datacamp.com/courses/understanding-data-engineering/">Understanding Data Engineering</a></li>
</ul>
<h3 id="google">Google</h3>
<ul>
<li><a href="https://www.cloudskillsboost.google/course_templates/53/">Building Batch Data Pipelines on Google Cloud</a></li>
<li><a href="https://www.cloudskillsboost.google/course_templates/52/">Building Resilient Streaming Analytics Systems on Google Cloud</a></li>
<li><a href="https://www.cloudskillsboost.google/course_templates/54/">Modernizing Data Lakes and Data Warehouses with Google Cloud</a></li>
<li><a href="https://www.cloudskillsboost.google/course_templates/72/">Preparing for the Google Cloud Professional Data Engineer Exam</a></li>
<li><a href="https://www.cloudskillsboost.google/course_templates/229/">Serverless Data Processing with Dataflow: Develop Pipelines</a></li>
<li><a href="https://www.cloudskillsboost.google/course_templates/218/">Serverless Data Processing with Dataflow: Foundations</a></li>
<li><a href="https://www.cloudskillsboost.google/course_templates/264/">Serverless Data Processing with Dataflow: Operations</a></li>
</ul>
<h3 id="udacity">Udacity</h3>
<ul>
<li><a href="https://www.udacity.com/course/data-engineer-nanodegree--nd027/">How to Become a Data Engineer</a></li>
</ul>
<h3 id="udemy">Udemy</h3>
<ul>
<li><a href="https://www.udemy.com/course/taming-big-data-with-apache-spark-hands-on/">Taming Big Data with Apache Spark and Python - Hands On!</a></li>
</ul>
<h3 id="whizlabs">Whizlabs</h3>
<ul>
<li><a href="https://www.whizlabs.com/learn/course/apache-kafka-fundamentals/297/">Apache Kafka Fundamentals</a></li>
<li><a href="https://www.whizlabs.com/databricks-certified-associate-developer-apache-spark/">Databricks Certified Associate Developer for Apache Spark (Python)</a></li>
<li><a href="https://www.whizlabs.com/databricks-certified-data-analyst-associate/">Databricks Certified Data Analyst Associate Certification</a></li>
<li><a href="https://www.whizlabs.com/databricks-certified-data-engineer-associate/">Databricks Certified Data Engineer Associate Certification</a></li>
<li><a href="https://www.whizlabs.com/databricks-certified-data-engineer-professional/">Databricks Certified Data Engineer Professional Certification</a></li>
<li><a href="https://www.whizlabs.com/learn/course/snowflake-snowpro-core-certification/384/">Snowflake SnowPro Core Certification</a></li>
</ul>
<h2 id="data-science">Data Science</h2>
<h3 id="a-cloud-guru-1">A Cloud Guru</h3>
<ul>
<li><a href="https://acloudguru.com/course/introduction-to-machine-learning/">Introduction to Machine Learning</a></li>
</ul>
<h3 id="coursera-1">Coursera</h3>
<ul>
<li><a href="https://www.coursera.org/specializations/data-science-with-databricks-for-data-analysts/">Data Science with Databricks for Data Analysts Specialization</a></li>
</ul>
<h3 id="datacamp-1">DataCamp</h3>
<ul>
<li><a href="https://www.datacamp.com/courses/introduction-to-data-science-in-python/">Introduction to Data Science in Python</a></li>
<li><a href="https://www.datacamp.com/courses/python-data-science-toolbox-part-1/">Python Data Science Toolbox (Part 1)</a></li>
</ul>
<h3 id="google-1">Google</h3>
<ul>
<li><a href="https://learndigital.withgoogle.com/digitalunlocked/course/data-science-foundations/">Data Science Foundations</a></li>
<li><a href="https://learndigital.withgoogle.com/digitalunlocked/course/data-science-with-python/">Data Science with Python</a></li>
<li><a href="https://www.cloudskillsboost.google/course_templates/3/">Google Cloud Big Data and Machine Learning Fundamentals</a></li>
<li><a href="https://learndigital.withgoogle.com/digitalunlocked/course/intro-to-tensorflow-for-deep-learning/">Intro to TensorFlow for Deep Learning</a></li>
<li><a href="https://learndigital.withgoogle.com/digitalunlocked/course/learn-python-basics-for-data-analysis/">Learn Python basics for data analysis</a></li>
<li><a href="https://learndigital.withgoogle.com/digitalunlocked/course/machine-learning-crash-course/">Machine Learning Crash Course</a></li>
<li><a href="https://www.cloudskillsboost.google/course_templates/55/">Smart Analytics, Machine Learning, and AI on Google Cloud</a></li>
</ul>
<h3 id="udemy-1">Udemy</h3>
<ul>
<li><a href="https://www.udemy.com/course/aws-machine-learning/">AWS Certified Machine Learning Specialty 2023 - Hands On!</a></li>
</ul>
<h3 id="whizlabs-1">Whizlabs</h3>
<ul>
<li><a href="https://www.whizlabs.com/learn/course/aws-certified-machine-learning-specialty/281/">AWS Certified Machine Learning Specialty</a></li>
<li><a href="https://www.whizlabs.com/databricks-certified-machine-learning-associate/">Databricks Certified Machine Learning Associate Certification</a></li>
<li><a href="https://www.whizlabs.com/databricks-certified-machine-learning-professional-certification/">Databricks Certified Machine Learning Professional Certification</a></li>
<li><a href="https://www.whizlabs.com/learn/course/data-science-with-python/379/">Introduction to Data Science with Python</a></li>
<li><a href="https://www.whizlabs.com/learn/course/tensorflow-for-deep-learning-with-python/1117/">TensorFlow for Deep Learning with Python</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>List of ETL Tools</title>
      <link>https://jamesm.blog/data-engineering/etl-tools/</link>
      <pubDate>Fri, 01 Jan 2021 06:51:25 +0100</pubDate>
      
      <guid>https://jamesm.blog/data-engineering/etl-tools/</guid>
      <description>&lt;h3 id=&#34;what-is-etl-&#34;&gt;What is ETL ?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;E&lt;/strong&gt;xtracting data from sources&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;T&lt;/strong&gt;ransforming data into data models&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;L&lt;/strong&gt;oading data into data warehouses&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;etl-tools&#34;&gt;ETL Tools&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.abinitio.com/&#34;&gt;Ab Initio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://aws.amazon.com/datapipeline/&#34;&gt;AWS Data Pipeline&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://aws.amazon.com/glue/&#34;&gt;AWS Glue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/en-us/azure/data-factory/&#34;&gt;Azure Data Factory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cloverdx.com/&#34;&gt;CloverDX&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.ibm.com/products/datastage&#34;&gt;Datastage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/dataflow&#34;&gt;Google Cloud Dataflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://hevodata.com/&#34;&gt;Hevo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.informatica.com/&#34;&gt;Informatica&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.integrate.io/&#34;&gt;Integrate&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.matillion.com/&#34;&gt;Matillion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://help.hitachivantara.com/Documentation/Pentaho/8.3/Products/Pentaho_Data_Integration&#34;&gt;Pentaho Data Integration (PDI)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.qlik.com/us/products/qlik-compose-data-warehouses&#34;&gt;Qlik Compose&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.sap.com/products/technology-platform/data-services.html&#34;&gt;SAP Data Services&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/en-us/sql/integration-services/sql-server-integration-services/&#34;&gt;SQL Server Integration Services (SSIS)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.stitchdata.com/&#34;&gt;Stitch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.talend.com/&#34;&gt;Talend&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
      <content:encoded><![CDATA[<h3 id="what-is-etl-">What is ETL ?</h3>
<ul>
<li><strong>E</strong>xtracting data from sources</li>
<li><strong>T</strong>ransforming data into data models</li>
<li><strong>L</strong>oading data into data warehouses</li>
</ul>
<h3 id="etl-tools">ETL Tools</h3>
<ul>
<li><a href="https://www.abinitio.com/">Ab Initio</a></li>
<li><a href="https://aws.amazon.com/datapipeline/">AWS Data Pipeline</a></li>
<li><a href="https://aws.amazon.com/glue/">AWS Glue</a></li>
<li><a href="https://learn.microsoft.com/en-us/azure/data-factory/">Azure Data Factory</a></li>
<li><a href="https://www.cloverdx.com/">CloverDX</a></li>
<li><a href="https://www.ibm.com/products/datastage">Datastage</a></li>
<li><a href="https://cloud.google.com/dataflow">Google Cloud Dataflow</a></li>
<li><a href="https://hevodata.com/">Hevo</a></li>
<li><a href="https://www.informatica.com/">Informatica</a></li>
<li><a href="https://www.integrate.io/">Integrate</a></li>
<li><a href="https://www.matillion.com/">Matillion</a></li>
<li><a href="https://help.hitachivantara.com/Documentation/Pentaho/8.3/Products/Pentaho_Data_Integration">Pentaho Data Integration (PDI)</a></li>
<li><a href="https://www.qlik.com/us/products/qlik-compose-data-warehouses">Qlik Compose</a></li>
<li><a href="https://www.sap.com/products/technology-platform/data-services.html">SAP Data Services</a></li>
<li><a href="https://learn.microsoft.com/en-us/sql/integration-services/sql-server-integration-services/">SQL Server Integration Services (SSIS)</a></li>
<li><a href="https://www.stitchdata.com/">Stitch</a></li>
<li><a href="https://www.talend.com/">Talend</a></li>
</ul>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
