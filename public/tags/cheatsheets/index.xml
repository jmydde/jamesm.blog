<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cheatsheets on jamesm.blog</title>
    <link>https://jamesm.blog/tags/cheatsheets/</link>
    <description>Recent content in Cheatsheets on jamesm.blog</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Sun, 25 Feb 2024 21:00:25 +0100</lastBuildDate>
    <atom:link href="https://jamesm.blog/tags/cheatsheets/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Databricks Cheat Sheets</title>
      <link>https://jamesm.blog/data-engineering/databricks-cheatsheets/</link>
      <pubDate>Sun, 25 Feb 2024 21:00:25 +0100</pubDate>
      <guid>https://jamesm.blog/data-engineering/databricks-cheatsheets/</guid>
      <description>&lt;h2 id=&#34;databricks-notebook-commands&#34;&gt;Databricks Notebook Commands&lt;/h2&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Command&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Purpose&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: left&#34;&gt;Example&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;%config&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Set configuration options for the notebook&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;%env&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Set environment variables&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;%fs&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Interact with the Databricks file system&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;%fs ls dbfs:/repo&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;%load&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Loads the contents of a file into a cell&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;%lsmagic&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;List all magic commands&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;%jobs&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Lists all running jobs&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;%matplotlib&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;sets up the matplotlib backend&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;%md&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Write Markdown text&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;%pip&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Install Python packages&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;%python&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Executes python code&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;%python dbutils.fs.rm(&amp;quot;/user/hive/warehouse/test/&amp;quot;, True)&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;%r&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Execute R code&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;%reload&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;reloads module contents&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;%run&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Executes a Python file or a notebook&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;%scala&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Executes scala code&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;%sh&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Executes shell commands on the cluster nodes&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;%sh git clone &lt;a href=&#34;https://github.com/repo/test&#34;&gt;https://github.com/repo/test&lt;/a&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;%sql&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Executes SQL queries&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;%who&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;Lists all the variables in the current scope&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: left&#34;&gt;&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;h3 id=&#34;accessing-files&#34;&gt;Accessing Files&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;/path/to/file&lt;/li&gt;&#xA;&lt;li&gt;dbfs:/path/to/file&lt;/li&gt;&#xA;&lt;li&gt;file:/path/to/file&lt;/li&gt;&#xA;&lt;li&gt;s3://path/to/file&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;copying-files&#34;&gt;Copying Files&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;%fs cp file:/&amp;lt;path&amp;gt; /Volumes/&amp;lt;catalog&amp;gt;/&amp;lt;schema&amp;gt;/&amp;lt;volume&amp;gt;/&amp;lt;path&amp;gt;&#xA;&#xA;%python dbutils.fs.cp(&amp;#34;file:/&amp;lt;path&amp;gt;&amp;#34;, &amp;#34;/Volumes/&amp;lt;catalog&amp;gt;/&amp;lt;schema&amp;gt;/&amp;lt;volume&amp;gt;/&amp;lt;path&amp;gt;&amp;#34;)&#xA;%python dbutils.fs.cp(&amp;#34;file:/databricks/driver/test&amp;#34;, &amp;#34;dbfs:/repo&amp;#34;, True)&#xA;&#xA;%sh cp /&amp;lt;path&amp;gt; /Volumes/&amp;lt;catalog&amp;gt;/&amp;lt;schema&amp;gt;/&amp;lt;volume&amp;gt;/&amp;lt;path&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;sql-statements-ddl&#34;&gt;SQL Statements (DDL)&lt;/h2&gt;&#xA;&lt;h3 id=&#34;create--use-schema&#34;&gt;Create &amp;amp; Use Schema&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;CREATE SCHEMA test;&#xA;CREATE SCHEMA custom LOCATION &amp;#39;dbfs:/custom&amp;#39;;&#xA;&#xA;USE SCHEMA test;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;create-table&#34;&gt;Create Table&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;CREATE TABLE test(col1 INT, col2 STRING, col3 STRING, col4 BIGINT, col5 INT, col6 FLOAT);&#xA;CREATE TABLE test AS SELECT * EXCEPT (_rescued_data) FROM read_files(&amp;#39;/repo/data/test.csv&amp;#39;);&#xA;CREATE TABLE test USING CSV LOCATION &amp;#39;/repo/data/test.csv&amp;#39;;&#xA;CREATE TABLE test USING CSV OPTIONS (header=&amp;#34;true&amp;#34;) LOCATION &amp;#39;/repo/data/test.csv&amp;#39;;&#xA;CREATE TABLE test AS SELECT * EXCEPT (_rescued_data) FROM read_files(&amp;#39;/repo/data/test.csv&amp;#39;);&#xA;CREATE TABLE test AS ...&#xA;CREATE TABLE test USING ...&#xA;&#xA;CREATE TABLE test(id INT, title STRING, col1 STRING, publish_time BIGINT, pages INT, price FLOAT)&#xA;COMMENT &amp;#39;This is comment for the table itself&amp;#39;;&#xA;&#xA;CREATE TABLE test AS&#xA;SELECT * EXCEPT (_rescued_data)&#xA;FROM read_files(&amp;#39;/repo/data/test.json&amp;#39;, format =&amp;gt; &amp;#39;json&amp;#39;);&#xA;&#xA;CREATE TABLE test_raw AS&#xA;SELECT * EXCEPT (_rescued_data)&#xA;FROM read_files(&amp;#39;/repo/data/test.csv&amp;#39;, sep =&amp;gt; &amp;#39;;&amp;#39;);&#xA;&#xA;CREATE TABLE custom_table_test LOCATION &amp;#39;dbfs:/custom-table&amp;#39;&#xA;AS SELECT * EXCEPT (_rescued_data) FROM read_files(&amp;#39;/repo/data/test.csv&amp;#39;);&#xA;&#xA;CREATE TABLE test PARTITIONED BY (col1)&#xA;AS SELECT * EXCEPT (_rescued_data) FROM read_files(&amp;#39;/repo/data/test.csv&amp;#39;)&#xA;&#xA;CREATE TABLE users(&#xA;firstname STRING,&#xA;lastname STRING,&#xA;full_name STRING GENERATED ALWAYS AS (concat(firstname, &amp;#39; &amp;#39;, lastname))&#xA;);&#xA;&#xA;CREATE OR REPLACE TABLE test AS SELECT * EXCEPT (_rescued_data) FROM read_files(&amp;#39;/repo/data/test.csv&amp;#39;);&#xA;CREATE OR REPLACE TABLE test AS SELECT * FROM json.`/repo/data/test.json`;&#xA;CREATE OR REPLACE TABLE test AS SELECT * FROM read_files(&amp;#39;/repo/data/test.csv&amp;#39;);&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;create-view&#34;&gt;Create View&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;CREATE VIEW view_test&#xA;AS SELECT * FROM test WHERE col1 = &amp;#39;test&amp;#39;;&#xA;&#xA;CREATE VIEW view_test&#xA;AS SELECT col1, col1&#xA;FROM test&#xA;JOIN test2 ON test.col2 == test2.col2;&#xA;&#xA;CREATE TEMP VIEW temp_test&#xA;AS SELECT * FROM test WHERE col1 = &amp;#39;test&amp;#39;;&#xA;&#xA;CREATE TEMP VIEW temp_test&#xA;AS SELECT * FROM read_files(&amp;#39;/repo/data/test.csv&amp;#39;);&#xA;&#xA;CREATE GLOBAL TEMP VIEW view_test&#xA;AS SELECT * FROM test WHERE col1 = &amp;#39;test&amp;#39;;&#xA;SELECT * FROM global_temp.view_test;&#xA;&#xA;CREATE TEMP VIEW jdbc_example USING JDBC&#xA;OPTIONS (&#xA;url &amp;#34;&amp;lt;jdbc-url&amp;gt;&amp;#34;,&#xA;dbtable &amp;#34;&amp;lt;table-name&amp;gt;&amp;#34;,&#xA;user &amp;#39;&amp;lt;username&amp;gt;&amp;#39;,&#xA;password &amp;#39;&amp;lt;password&amp;gt;&amp;#39;);&#xA;&#xA;CREATE OR REPLACE TEMP VIEW test AS SELECT * FROM delta.`&amp;lt;logpath&amp;gt;`;&#xA;&#xA;CREATE VIEW event_log_raw AS SELECT * FROM event_log(&amp;#34;&amp;lt;pipeline-id&amp;gt;&amp;#34;);&#xA;&#xA;CREATE OR REPLACE TEMP VIEW test_view&#xA;AS SELECT test.col1 AS col1 FROM test_table&#xA;WHERE col1 = &amp;#39;value1&amp;#39; ORDER BY timestamp DESC LIMIT 1;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;drop&#34;&gt;Drop&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;DROP TABLE test;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;describe&#34;&gt;Describe&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;SHOW TABLES;&#xA;&#xA;DESCRIBE EXTENDED test;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;sql-statements-dml&#34;&gt;SQL Statements (DML)&lt;/h2&gt;&#xA;&lt;h3 id=&#34;select&#34;&gt;Select&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;SELECT * FROM csv.`/repo/data/test.csv`;&#xA;SELECT * FROM read_files(&amp;#39;/repo/data/test.csv&amp;#39;);&#xA;SELECT * FROM read_files(&amp;#39;/repo/data/test.csv&amp;#39;, format =&amp;gt; &amp;#39;csv&amp;#39;, header =&amp;gt; &amp;#39;true&amp;#39;, sep =&amp;gt; &amp;#39;,&amp;#39;)&#xA;SELECT * FROM json.`/repo/data/test.json`;&#xA;SELECT * FROM json.`/repo/data/*.json`;&#xA;SELECT * FROM test WHERE year(from_unixtime(test_time)) &amp;gt; 1900;&#xA;SELECT * FROM test WHERE title LIKE &amp;#39;%a%&amp;#39;&#xA;SELECT * FROM test WHERE title LIKE &amp;#39;a%&amp;#39;&#xA;SELECT * FROM test WHERE title LIKE &amp;#39;%a&amp;#39;&#xA;SELECT * FROM test TIMESTAMP AS OF &amp;#39;2024-01-01T00:00:00.000Z&amp;#39;;&#xA;SELECT * FROM test VERSION AS OF 2;&#xA;SELECT * FROM test@v2;&#xA;SELECT * FROM event_log(&amp;#34;&amp;lt;pipeline-id&amp;gt;&amp;#34;);&#xA;&#xA;SELECT count(*) FROM VALUES (NULL), (10), (10) AS example(col);&#xA;SELECT count(col) FROM VALUES (NULL), (10), (10) AS example(col);&#xA;SELECT count_if(col1 = &amp;#39;test&amp;#39;) FROM test;&#xA;SELECT from_unixtime(test_time) FROM test;&#xA;SELECT cast(test_time / 1 AS timestamp) FROM test;&#xA;SELECT cast(cast(test_time AS BIGINT) AS timestamp) FROM test;&#xA;SELECT element.sub_element FROM test;&#xA;SELECT flatten(array(array(1, 2), array(3, 4)));&#xA;&#xA;SELECT * FROM (&#xA;  SELECT col1, col2 FROM test&#xA;) &#xA;PIVOT (&#xA;  sum(col1) for col2 in (&amp;#39;item1&amp;#39;,&amp;#39;item2&amp;#39;)&#xA;);&#xA;&#xA;SELECT *, CASE&#xA;WHEN col1 &amp;gt; 10 THEN &amp;#39;value1&amp;#39;&#xA;ELSE &amp;#39;value2&amp;#39;&#xA;END&#xA;FROM test;&#xA;&#xA;SELECT * FROM test ORDER BY (CASE&#xA;WHEN col1 &amp;gt; 10 THEN col2&#xA;ELSE col3&#xA;END);&#xA;&#xA;WITH t(col1, col2) AS (SELECT 1, 2)&#xA;SELECT * FROM t WHERE col1 = 1;&#xA;&#xA;SELECT details:flow_definition.output_dataset as output_dataset,&#xA;       details:flow_definition.input_datasets as input_dataset&#xA;FROM   event_log_raw, latest_update&#xA;WHERE  event_type = &amp;#39;flow_definition&amp;#39; AND origin.update_id = latest_update.id;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;insert&#34;&gt;Insert&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;INSERT OVERWRITE test SELECT * FROM read_files(&amp;#39;/repo/data/test.csv&amp;#39;);&#xA;&#xA;INSERT INTO test(col1, col2) VALUES (&amp;#39;value1&amp;#39;, &amp;#39;value2&amp;#39;);&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;merge-into&#34;&gt;Merge Into&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;MERGE INTO test USING test_to_delete&#xA;ON test.col1 = test_to_delete.col1&#xA;WHEN MATCHED THEN DELETE;&#xA;&#xA;MERGE INTO test USING test_to_update&#xA;ON test.col1 = test_to_update.col1&#xA;WHEN MATCHED THEN UPDATE SET *;&#xA;&#xA;MERGE INTO test USING test_to_insert&#xA;ON test.col1 = test_to_insert.col1&#xA;WHEN NOT MATCHED THEN INSERT *;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;copy-into&#34;&gt;Copy Into&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;COPY INTO test&#xA;FROM &amp;#39;/repo/data&amp;#39;&#xA;FILEFORMAT = CSV&#xA;FILES = (&amp;#39;test.csv&amp;#39;)&#xA;FORMAT_OPTIONS(&amp;#39;header&amp;#39; = &amp;#39;true&amp;#39;, &amp;#39;inferSchema&amp;#39; = &amp;#39;true&amp;#39;);&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;delta-lake-statements&#34;&gt;Delta Lake Statements&lt;/h2&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;DESCRIBE HISTORY test;&#xA;DESCRIBE HISTORY test LIMIT 1;&#xA;&#xA;INSERT INTO test SELECT * FROM test@v2 WHERE id = 3;&#xA;&#xA;OPTIMIZE test;&#xA;OPTIMIZE test ZORDER BY col1;&#xA;&#xA;RESTORE TABLE test TO VERSION AS OF 0;&#xA;&#xA;SELECT * FROM test TIMESTAMP AS OF &amp;#39;2024-01-01T00:00:00.000Z&amp;#39;;&#xA;SELECT * FROM test VERSION AS OF 2;&#xA;SELECT * FROM test@v2;&#xA;&#xA;VACUUM test;&#xA;VACUUM test RETAIN 240 HOURS;&#xA;&#xA;%fs ls dbfs:/user/hive/warehouse/test/_delta_log&#xA;%python spark.conf.set(&amp;#34;spark.databricks.delta.retentionDurationCheck.enabled&amp;#34;, &amp;#34;false&amp;#34;)&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;delta-live-table-statements&#34;&gt;Delta Live Table Statements&lt;/h2&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;CREATE OR REFRESH LIVE TABLE test_raw&#xA;AS SELECT * FROM json.`/repo/data/test.json`;&#xA;&#xA;CREATE OR REFRESH STREAMING TABLE test&#xA;AS SELECT * FROM STREAM read_files(&amp;#39;/repo/data/test*.json&amp;#39;);&#xA;&#xA;CREATE OR REFRESH LIVE TABLE test_cleaned&#xA;AS SELECT col1, col2, col3, col4 FROM live.test_raw;&#xA;&#xA;CREATE OR REFRESH LIVE TABLE recent_test&#xA;AS SELECT col1, col2 FROM live.test2 ORDER BY creation_time DESC LIMIT 10;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;fuctions&#34;&gt;Fuctions&lt;/h2&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;CREATE OR REPLACE FUNCTION test_function(temp DOUBLE)&#xA;RETURNS DOUBLE&#xA;RETURN (col1 - 10);&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;auto-loader&#34;&gt;Auto Loader&lt;/h2&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;%python&#xA;&#xA;spark.readStream.format(&amp;#34;cloudFiles&amp;#34;)\&#xA;  .option(&amp;#34;cloudFiles.format&amp;#34;, &amp;#34;json&amp;#34;)\&#xA;  .option(&amp;#34;cloudFiles.schemaLocation&amp;#34;, &amp;#34;/autoloader-schema&amp;#34;)\&#xA;  .option(&amp;#34;pathGlobFilter&amp;#34;, &amp;#34;test*.json&amp;#34;)\&#xA;  .load(&amp;#34;/repo/data&amp;#34;)\&#xA;  .writeStream\&#xA;  .option(&amp;#34;mergeSchema&amp;#34;, &amp;#34;true&amp;#34;)\&#xA;  .option(&amp;#34;checkpointLocation&amp;#34;, &amp;#34;/autoloader-checkpoint&amp;#34;)\&#xA;  .start(&amp;#34;demo&amp;#34;)&#xA;&#xA;%fs head /autoloader-schema/_schemas/0&#xA;&#xA;CREATE OR REFRESH STREAMING TABLE test&#xA;AS SELECT * FROM&#xA;cloud_files(&#xA;&amp;#39;/repo/data&amp;#39;,&#xA;&amp;#39;json&amp;#39;,&#xA;map(&amp;#34;cloudFiles.inferColumnTypes&amp;#34;, &amp;#34;true&amp;#34;, &amp;#34;pathGlobFilter&amp;#34;, &amp;#34;test*.json&amp;#34;)&#xA;);&#xA;&#xA;CONSTRAINT positive_timestamp EXPECT (creation_time &amp;gt; 0)&#xA;CONSTRAINT positive_timestamp EXPECT (creation_time &amp;gt; 0) ON VIOLATION DROP ROW&#xA;CONSTRAINT positive_timestamp EXPECT (creation_time &amp;gt; 0) ON VIOLATION FAIL UPDATE&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;cdc-statements&#34;&gt;CDC Statements&lt;/h2&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;APPLY CHANGES INTO live.target&#xA;  FROM stream(live.cdc_source)&#xA;  KEYS (col1)&#xA;  APPLY AS DELETE WHEN col2 = &amp;#34;DELETE&amp;#34;&#xA;  SEQUENCE BY col3&#xA;  COLUMNS * EXCEPT (col);&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;security-statements&#34;&gt;Security Statements&lt;/h2&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;GRANT &amp;lt;privilege&amp;gt; ON &amp;lt;object_type&amp;gt; &amp;lt;object_name&amp;gt; TO &amp;lt;user_or_group&amp;gt;;&#xA;GRANT SELECT ON TABLE test TO `databricks@degols.net`;&#xA;&#xA;REVOKE &amp;lt;privilege&amp;gt; ON &amp;lt;object_type&amp;gt; &amp;lt;object_name&amp;gt; FROM `test@gmail.com&amp;#39;;&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;links&#34;&gt;Links&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/en/index.html&#34;&gt;Databricks&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/en/sql/language-manual/index.html&#34;&gt;SQL Language Reference&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/en/getting-started/best-practices.html&#34;&gt;Cheat Sheets&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/en/cheat-sheet/compute.html&#34;&gt;Compute creation cheat sheet&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/en/cheat-sheet/administration.html&#34;&gt;Platform administration cheat sheet&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/en/cheat-sheet/jobs.html&#34;&gt;Production job scheduling cheat sheet&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/en/getting-started/best-practices.html&#34;&gt;Best Practices&lt;/a&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/en/delta/best-practices.html&#34;&gt;Delta Lake best practices&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/en/machine-learning/automl-hyperparam-tuning/hyperopt-best-practices.html&#34;&gt;Hyperparameter tuning with Hyperopt&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/en/machine-learning/train-model/dl-best-practices.html&#34;&gt;Deep learning in Databricks&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/en/machine-learning/mlops/mlops-workflow.html&#34;&gt;Recommendations for MLOps&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/en/data-governance/unity-catalog/best-practices.html&#34;&gt;Unity Catalog best practices&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/en/compute/cluster-config-best-practices.html&#34;&gt;Cluster configuration best practices&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://docs.databricks.com/en/compute/pool-best-practices.html&#34;&gt;Instance pool configuration best practices&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Other&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mayur-saparia7.medium.com/databricks-cheat-sheet-1-a0d3e0f70065&#34;&gt;Databricks Cheat Sheet 1&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://grabngoinfo.com/databricks-notebook-markdown-cheat-sheet/&#34;&gt;Databricks Notebook Markdown Cheat Sheet&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
  </channel>
</rss>
